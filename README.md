# A-MultiHop-Rag-for-Nepali-Constitution


# ðŸ§  Multi-hop RAG Pipeline using LLaMA-3 + Qdrant + Groq

This project demonstrates a **multi-hop Retrieval-Augmented Generation (RAG)** pipeline using:

- **LLaMA-3 8B via Groq API** (for ultra-fast inference)
- **LangChain** (for chaining and orchestration)
- **Qdrant** (for vector-based retrieval)
- **BAAI bge-small embeddings** (for semantic chunking)
- **Gradio** (for an interactive interface)

---

## ðŸš€ Features

âœ… Multi-hop QA: Decomposes complex questions into multiple sub-questions  
âœ… Fast LLM Inference: Powered by Groqâ€™s blazing-fast LLaMA-3 API  
âœ… Vector Search: Retrieves context chunks from Qdrant DB  
âœ… Modular: Clean, reusable agents (`decomposer`, `retriever`, `synthesizer`)  
âœ… GPU/Accelerated: TinyLlama (optional) runs on local GPU  
âœ… Gradio Frontend: Simple UI for testing and demos

---

## ðŸ“ Project Structure

```bash
.
â”œâ”€â”€ rag/
â”‚   â”œâ”€â”€ decomposer.py       # Breaks down complex queries
â”‚   â”œâ”€â”€ retriever.py        # Uses HuggingFace + Qdrant for search
â”‚   â”œâ”€â”€ synthesizer.py      # Synthesizes final answer from sub-answers
â”‚   â”œâ”€â”€ llm_provider.py     # Loads LLaMA-3 via Groq API
â”‚   â”œâ”€â”€ ingest.py           # Loads your documents into Qdrant
â”‚   â””â”€â”€ multi_hop_agent.py  # Full agent logic (decompose â†’ retrieve â†’ synthesize)
â”œâ”€â”€ interface/
â”‚   â””â”€â”€ app.py              # Gradio app
â”œâ”€â”€ alltest.py              # Testing script for each agent
â””â”€â”€ README.md               # You're here

## ðŸ“¦ Requirements


langchain>=0.2.0
langchain-community>=0.0.27
langchain-core>=0.1.50
langchain-openai>=0.1.6
langchain-huggingface>=0.0.3
langchain-qdrant>=0.1.2

# Groq-compatible client
openai>=1.30.1  # For Groq API calls (OpenAI-compatible interface)

# Embeddings & Transformers
transformers>=4.41.1
accelerate>=0.30.1
torch>=2.2.2

# Qdrant client for ingestion/retrieval
qdrant-client>=1.9.0


# Gradio UI
gradio>=4.44.1

# Utilities
python-dotenv>=1.0.1



âœ… TODO
 Add GPU support for local TinyLlama

 Replace OpenAI with Groq

 Add sub-question limit

 Support Mixtral or Gemma (via Groq)

 Add streaming output to Gradio

## ðŸ“¬ Contact

For questions, feedback, or collaboration:

**Email:** [gautamaayush305@gmail.com](mailto:gautamaayush305@gmail.com)

Feel free to reach out!
