# A-MultiHop-Rag-for-Nepali-Constitution


# ğŸ§  Multi-hop RAG Pipeline using LLaMA-3 + Qdrant + Groq

This project demonstrates a **multi-hop Retrieval-Augmented Generation (RAG)** pipeline using:

- **LLaMA-3 8B via Groq API** (for ultra-fast inference)
- **LangChain** (for chaining and orchestration)
- **Qdrant** (for vector-based retrieval)
- **BAAI bge-small embeddings** (for semantic chunking)
- **Gradio** (for an interactive interface)

---

## ğŸš€ Features

âœ… Multi-hop QA: Decomposes complex questions into multiple sub-questions  
âœ… Fast LLM Inference: Powered by Groqâ€™s blazing-fast LLaMA-3 API  
âœ… Vector Search: Retrieves context chunks from Qdrant DB  
âœ… Modular: Clean, reusable agents (`decomposer`, `retriever`, `synthesizer`)  
âœ… GPU/Accelerated: TinyLlama (optional) runs on local GPU  
âœ… Gradio Frontend: Simple UI for testing and demos

---

## ğŸ“ Project Structure

```bash
.
â”œâ”€â”€ rag/
â”‚   â”œâ”€â”€ decomposer.py       # Breaks down complex queries
â”‚   â”œâ”€â”€ retriever.py        # Uses HuggingFace + Qdrant for search
â”‚   â”œâ”€â”€ synthesizer.py      # Synthesizes final answer from sub-answers
â”‚   â”œâ”€â”€ llm_provider.py     # Loads LLaMA-3 via Groq API
â”‚   â”œâ”€â”€ ingest.py           # Loads your documents into Qdrant
â”‚   â””â”€â”€ multi_hop_agent.py  # Full agent logic (decompose â†’ retrieve â†’ synthesize)
â”œâ”€â”€ interface/
â”‚   â””â”€â”€ app.py              # Gradio app
â”œâ”€â”€ alltest.py              # Testing script for each agent
â””â”€â”€ README.md               # You're here
